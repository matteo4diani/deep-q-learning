{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "mount_file_id": "1LmIgiaTlfQ7eRmbBIg2YjU5XYm7pRlJq",
      "authorship_tag": "ABX9TyN4TDc8g3I9D7KVMpjrifV1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matteo4diani/deep-q-learning/blob/main/deepqlearning_atari.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "## Q-Learning\n",
        "\n",
        "This notebook contains a replication of results in [\"Human-level control through deep reinforcement learning\"](https://doi.org/10.1038/nature14236), [\"Playing Atari with deep reinforcement learning\"](https://www.deepmind.com/publications/playing-atari-with-deep-reinforcement-learning), and [\"Deep reinforcement learning with Double Q-Learning\"](\n",
        "https://doi.org/10.48550/arXiv.1509.06461) by Google DeepMind members and collaborators.\n",
        "\n",
        "The critical innovation in these papers is predicting the Q-Values for the game at hand (needed for the loss calculations of the player agent network) with a neural network (called _target network_) that copies its weights (with a set periodicity represented by the __TARGET_UPDATE_FREQ__ constant) from the _online network_ that plays the game by choosing actions at each time-step.\n",
        "\n",
        "The _online_ and _target_ networks share the same architecture and differ only in their weights (_target_ lags behind _online_ until updated every __TARGET_UPDATE_FREQ__ steps).\n",
        "\n",
        "## Double Q-Learning\n",
        "\n",
        "A _Double Q-Network_ uses the player agent (online) network to compute its own loss function: best Q-Values indices are drawn from the online network (as indices in the Q-Values tensor correspond to actions, this phase can be seen as action selection) but the actual Q-Values for the actions are predicted using the _target network_, as previously defined.\n",
        "\n",
        "## Implementation details\n",
        "\n",
        "The preprocessing steps detailed in the papers (convert to gray-scale, frame-stacking, etc.) are implemented in the ___deep-q-learning-utils___ repository: a collection of PyTorch and OpenAI Baselines wrappers by GitHub user __@brtorch__.  \n",
        "\n",
        "Deep Q Learning on Atari games produce memory-heavy models, thus we serialize weights to disks so we can resume training after OOM events (set __RELOAD_PARAMS__ constant to __True__).\n",
        "\n",
        "This implementation uses a GPU with CUDA if available.\n",
        "\n"
      ],
      "metadata": {
        "id": "I5dGzioX9rqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Usage\n",
        "\n",
        "1. Run all the cells under `Environment Setup` \n",
        "2. Run all the cells in `Project Imports`\n",
        "3. Configure model training, checkpoints and logs in the `Constants` cell\n",
        "4. Ignore cells in `Persistence Clean-up`. If you need to clean up logs and serialized network parameters, replace the paths in the cell with the paths to your logs\n",
        "5. If you want to use Google Drive, set `USE_DRIVE` to `True` and run the `Mount Google Drive` cell"
      ],
      "metadata": {
        "id": "nxoWXSq1gl_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Setup\n",
        "\n"
      ],
      "metadata": {
        "id": "Rvnw0S_yNDGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installs for:\n",
        "# ML framework (pytorch)\n",
        "# Serialization (msgpack)\n",
        "# Atari env (gym)\n",
        "# Tensorflow introspection and visualization (tensorboard)\n",
        "!pip install torch gym\n",
        "!pip install autorom[accept-rom-license]\n",
        "!sudo apt-get install zlib1g-dev cmake\n",
        "!pip install 'msgpack==1.0.2' gym[atari] tensorboard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0cDzQB6OE0W",
        "outputId": "29189546-47b6-43d7-fce8-2d11b47af001"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.25.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym) (3.8.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: autorom[accept-rom-license] in /usr/local/lib/python3.7/dist-packages (0.4.2)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]) (5.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]) (7.1.2)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]) (0.4.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->autorom[accept-rom-license]) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]) (3.0.4)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.2).\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2.2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 32 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: msgpack==1.0.2 in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.7/dist-packages (0.25.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.21.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.35.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.2.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.4.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.37.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.0.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.17.3)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.48.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (57.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: ale-py~=0.7.5 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (0.7.5)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[atari]) (5.9.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-9EsR3Uqmdr3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35464d4e-1adf-46cc-8698-068b6ffe1179"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'deep-q-learning-utils'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 30 (delta 6), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (30/30), done.\n"
          ]
        }
      ],
      "source": [
        "# Clone PyTorch/StableBaselines wrappers\n",
        "!git clone https://github.com/matteo4diani/deep-q-learning-utils.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Put the downloaded repo on path so we can import custom libraries into Colab\n",
        "import sys\n",
        "sys.path.insert(0,'/content/deep-q-learning-utils')"
      ],
      "metadata": {
        "id": "6Rvzb2eNJLY_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Imports"
      ],
      "metadata": {
        "id": "MLEXZszENvvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.nn.modules.activation import ReLU\n",
        "\n",
        "import torch\n",
        "import gym\n",
        "from collections import deque\n",
        "import itertools\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "from pathlib import Path\n",
        "\n",
        "from pytorch_wrappers import make_atari_deepmind, BatchedPytorchFrameStack, PytorchLazyFrames\n",
        "from baselines_wrappers import Monitor, DummyVecEnv, SubprocVecEnv\n",
        "\n",
        "import msgpack\n",
        "from msgpack_numpy import patch as msgpack_numpy_patch\n",
        "msgpack_numpy_patch()\n",
        "# Load TensorBoard extension\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "QMeJdJG3JLfz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constants"
      ],
      "metadata": {
        "id": "GcXfQjQlN2dA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##################################\n",
        "# Model Constants (taken from:   #\n",
        "# \"Human-level control through   #\n",
        "# deep reinforcement learning\")  #\n",
        "# with some additions (Double Q) #\n",
        "##################################\n",
        "\n",
        "# Use Double-Q Learning as detailed in:\n",
        "# \"Deep Reinforcement Learning with Double-Q Learning\"\n",
        "USE_DOUBLE = True\n",
        "# Discount rate\n",
        "GAMMA = 0.99\n",
        "# How many transitions to sample from\n",
        "BATCH_SIZE = 32\n",
        "# How many transitions we're gonna store before overwrite\n",
        "BUFFER_SIZE = int(1e6)\n",
        "# How many transitions to accumulate before we start the actual training\n",
        "MIN_REPLAY_SIZE = 50000\n",
        "# Starting value of epsilon (probability of taking random action)\n",
        "EPSILON_START = 1.0\n",
        "# Final value of epsilon\n",
        "EPSILON_END = 0.1\n",
        "# Number of steps taken for EPSILON_START to become EPSILON_END\n",
        "EPSILON_DECAY = int(1e6)\n",
        "# Number of batch elements (environments created)\n",
        "N_ENVS = 4\n",
        "# Periodicity for target updates with the online values\n",
        "TARGET_UPDATE_FREQ = 10000 // N_ENVS\n",
        "# Learning Rate\n",
        "LEARNING_RATE = 5e-5\n",
        "# If True force taking action 1 at the start of each round to initiate gameplay.\n",
        "# Setting FORCE_START to True may alter the learning process and sho\n",
        "FORCE_START = False\n",
        "\n",
        "#####################\n",
        "# Utility Constants #\n",
        "#####################\n",
        "\n",
        "SAVE_PATHS = {True:  '/content/gdrive/MyDrive/deep-q-learning-atari/checkpoints/atari_model.pack', \n",
        "              False: 'checkpoints/atari_model.pack'}\n",
        "LOG_DIRS = {True: '/content/gdrive/MyDrive/deep-q-learning-atari/tensorboard/atari_model',\n",
        "            False: 'tensorboard/atari_model'}\n",
        "# Use your personal Google Drive for parameter serialization and logs\n",
        "USE_DRIVE = True\n",
        "# Reload parameters from disk/drive\n",
        "RELOAD_PARAMS = True\n",
        "# Path for network parameters serialization\n",
        "SAVE_PATH = SAVE_PATHS[USE_DRIVE]\n",
        "SAVE_INTERVAL = 10000\n",
        "# Path for TensorBoard logging\n",
        "LOG_DIR = LOG_DIRS[USE_DRIVE]\n",
        "LOG_INTERVAL = 1000"
      ],
      "metadata": {
        "id": "ACMeYFuxN2qT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive"
      ],
      "metadata": {
        "id": "0aGWK5cKgc6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount google drive so we can store data between runs\n",
        "from google.colab import drive\n",
        "if USE_DRIVE:\n",
        "  drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSBlsAg5ihWu",
        "outputId": "4947f5db-a860-416e-ba2e-19f7a6680909"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Persistence Clean-up"
      ],
      "metadata": {
        "id": "KSvWXaN_fp3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove persistence directories\n",
        "import shutil\n",
        "shutil.rmtree('/content/gdrive/MyDrive/deep-q-learning-atari/checkpoints', ignore_errors=True)\n",
        "shutil.rmtree('/content/gdrive/MyDrive/deep-q-learning-atari/tensorboard', ignore_errors=True)"
      ],
      "metadata": {
        "id": "5LNcaVIWOzXi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network Definition"
      ],
      "metadata": {
        "id": "vaTIZATCN3AF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "# Network Architecture #\n",
        "########################\n",
        "\n",
        "def nature_cnn(observation_space, depths=(32, 64, 64), final_layer=512):\n",
        "  \"\"\"\n",
        "  CNN architecture as defined in 'Human-level Control through \n",
        "  deep reinforcement learning'\n",
        "  \"\"\"\n",
        "  # Get the number of input channels\n",
        "  n_input_channels = observation_space.shape[0]\n",
        "\n",
        "  cnn = nn.Sequential(\n",
        "      nn.Conv2d(n_input_channels, depths[0], kernel_size=8, stride=4),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(depths[0], depths[1], kernel_size=4, stride=2),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(depths[1], depths[2], kernel_size=3, stride=1),\n",
        "      nn.ReLU(),\n",
        "      nn.Flatten())\n",
        "  # Compute shape by doing one forward pass through the cnn\n",
        "  # and looking at the output shape of the tensor\n",
        "  with torch.no_grad():\n",
        "    # We are not passing this tensor to the gpu:\n",
        "    # Our NNs will still be on the CPU when nature_cnn(...) is called.\n",
        "    n_flatten = cnn(torch.as_tensor(observation_space.sample()[None]).float()).shape[1]\n",
        "    out = nn.Sequential(cnn, nn.Linear(n_flatten, final_layer), nn.ReLU())\n",
        "  \n",
        "  return out\n",
        "\n",
        "# Class representing the neural network, implements PyTorch nn.Module interface\n",
        "class Network(nn.Module):\n",
        "  def __init__(self, env, device, double):\n",
        "    super().__init__()\n",
        "    # Use Double-Q Learning\n",
        "    self.double = double\n",
        "    # Enable GPU support with explicit tensor/model allocation\n",
        "    self.device = device\n",
        "    # Number of actions available to the agent\n",
        "    self.num_actions = env.action_space.n\n",
        "    # Get Nature CNN instance\n",
        "    conv_net = nature_cnn(env.observation_space)\n",
        "    # Create network stacking the Nature CNN and a last layer \n",
        "    # dependent on the game environment \n",
        "    # (different num_actions, not knowable a-priori)\n",
        "    self.net = nn.Sequential(conv_net, nn.Linear(512, self.num_actions))\n",
        "  # Forward function is part of the interface for nn.Module\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "  def act(self, obses, epsilon):\n",
        "    # Convert observations to PyTorch tensor\n",
        "    obses_tensor = torch.as_tensor(obses, \n",
        "                                   dtype=torch.float32, \n",
        "                                   device=self.device)\n",
        "    \n",
        "    # PyTorch already expects a batch of samples so we pass the tensor as-is\n",
        "    # and we get a prediction from the Q-Network\n",
        "    q_values = self(obses_tensor)\n",
        "\n",
        "    # Get argmaxes of actions with best q\n",
        "    max_q_indices = torch.argmax(q_values, dim=1)\n",
        "    # Cast tensor into list of ints\n",
        "    actions = max_q_indices.detach().tolist()\n",
        "\n",
        "    # Implement epsilon-greedy policy.\n",
        "    # We get P(random action) = epsilon by P(randint(0,1) <= epsilon) = epsilon\n",
        "    for i in range(len(actions)):\n",
        "      rnd_sample = random.random()\n",
        "      if rnd_sample <= epsilon:\n",
        "        actions[i] = random.randint(0, self.num_actions - 1)\n",
        "    \n",
        "    return actions\n",
        "\n",
        "  def compute_loss(self, transitions, target_net):\n",
        "    # Comb data and turn to numpy array for faster runs\n",
        "    obses = [t[0] for t in transitions]\n",
        "    actions = np.asarray([t[1] for t in transitions])\n",
        "    rewards = np.asarray([t[2] for t in transitions])\n",
        "    dones = np.asarray([t[3] for t in transitions])\n",
        "    new_obses = [t[4] for t in transitions]\n",
        "    \n",
        "    # If using frame-stacking use helper get_frames() to get numpy compliant obj\n",
        "    if isinstance(obses[0], PytorchLazyFrames):\n",
        "      obses = np.stack([o.get_frames() for o in obses])\n",
        "      new_obses = np.stack([o.get_frames() for o in new_obses])\n",
        "    else:\n",
        "      obses = np.asarray(obses)\n",
        "      new_obses = np.asarray(new_obses)\n",
        "\n",
        "    # Turn to PyTorch tensor\n",
        "    obses_tensor = torch.as_tensor(obses, \n",
        "                                   dtype=torch.float32, \n",
        "                                   device=self.device)\n",
        "    # We unsqueeze(-1) to wrap each action/rew/... in an additional dimension\n",
        "    actions_tensor = torch.as_tensor(actions,\n",
        "                                     dtype=torch.int64,\n",
        "                                     device=self.device).unsqueeze(-1)\n",
        "    rewards_tensor = torch.as_tensor(rewards, \n",
        "                                     dtype=torch.float32,\n",
        "                                     device=self.device).unsqueeze(-1)\n",
        "    dones_tensor = torch.as_tensor(dones, \n",
        "                                   dtype=torch.float32,\n",
        "                                   device=self.device).unsqueeze(-1)\n",
        "    new_obses_tensor = torch.as_tensor(new_obses, \n",
        "                                       dtype=torch.float32,\n",
        "                                       device=self.device)\n",
        "    with torch.no_grad():\n",
        "      if self.double:\n",
        "        # We modify the network to use the online net for action selection\n",
        "        # And the target net to compute Q-Values of actions.\n",
        "        # Taken from \"Deep Reinforcement Learning with Double-Q Learning\"\n",
        "        # By H. van Hasselt, A. Guez, and D. Silver from Google DeepMind.\n",
        "        targets_online_q_values = self(new_obses_tensor)\n",
        "        targets_online_best_q_indices = targets_online_q_values.argmax(dim=1, \n",
        "                                                                       keepdim=True)\n",
        "        targets_target_q_values = target_net(new_obses_tensor)\n",
        "        targets_selected_q_values = torch.gather(input=targets_target_q_values,\n",
        "                                                 dim=1,\n",
        "                                                 index=targets_online_best_q_indices)\n",
        "        targets = rewards_tensor + GAMMA * (1 - dones_tensor) * targets_selected_q_values\n",
        "\n",
        "      else:\n",
        "        # Compute targets for loss function\n",
        "        # We use the target net to predict target q-values for new obses\n",
        "        # For each new observation we have a set of q-values\n",
        "        # We need to condense this set to the one highest q-value per observation\n",
        "        # (N.B. pytorch tensors .max() return argmax at index 1)\n",
        "        target_q_values = target_net(new_obses_tensor)\n",
        "        max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]\n",
        "        # Compute r + gamma*max(Q) (\"if done -> r\" is obtained via \"1 - dones_tensor\")\n",
        "        targets = rewards_tensor + GAMMA * (1 - dones_tensor) * max_target_q_values\n",
        "\n",
        "    # Compute Loss\n",
        "    q_values = self(obses_tensor)\n",
        "\n",
        "    # Get q-values for the actions we took\n",
        "    action_q_values = torch.gather(input=q_values, dim=1, index=actions_tensor)\n",
        "\n",
        "    # Compute l1 loss\n",
        "    loss = nn.functional.smooth_l1_loss(action_q_values, targets)\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def save(self, save_path):\n",
        "    \"\"\"Serialize network parameters to disk or Google Drive\"\"\"\n",
        "\n",
        "    # We call .cpu() to transfer the tensor\n",
        "    # from the gpu when converting to np array\n",
        "    params = {k: t.detach().cpu().numpy() for k, t in self.state_dict().items()}\n",
        "    # Serialize network parameter dictionary with msgpack\n",
        "    params_data = msgpack.dumps(params)\n",
        "\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "    with open(save_path, 'wb') as f:\n",
        "      f.write(params_data)\n",
        "\n",
        "  def load(self, load_path):\n",
        "    \"\"\"Load network parameters from disk or Google Drive\"\"\"\n",
        "    if not os.path.exists(load_path):\n",
        "      raise FileNotFoundError(load_path)\n",
        "\n",
        "    with open(load_path, 'rb') as f:\n",
        "      params_numpy = msgpack.load(f)\n",
        "      # Convert to PyTorch tensors and load into network\n",
        "      params = {k: torch.as_tensor(v, device=self.device) for k,v in params_numpy.items()}\n",
        "      self.load_state_dict(params)"
      ],
      "metadata": {
        "id": "wvFAh-pXN3S4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "J4QiJ5V4OoGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############\n",
        "# Model Setup #\n",
        "###############\n",
        "\n",
        "# Enable GPU support\n",
        "device = torch.device('cuda:0') if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Load Breakout environment\n",
        "# We use a custom wrapper made by @brthor\n",
        "# The wrapper applies all the preprocessing steps described in\n",
        "# \"Human-level control through deep reinforcement learning\"\n",
        "# before the agent sees the observation.\n",
        "# It also transforms [Height, Width, Channel] -> [C, H, W] (PyTorch format)\n",
        "# Where H, W identify the pixel and channel is R, G or B.\n",
        "# We wrap make_atari_deepmind in a Monitor object that enriches the info\n",
        "# returned by env.step()\n",
        "make_env = lambda: Monitor(make_atari_deepmind('BreakoutNoFrameskip-v4',\n",
        "                                               scale_values=True), \n",
        "                           allow_early_resets=True)\n",
        "\n",
        "# Double configuration for VecEnv: sequential (dummy) and parallel (subproc)\n",
        "vec_env = DummyVecEnv([make_env for _ in range(N_ENVS)])\n",
        "#env = SubprocVecEnv([make_env for _ in range(N_ENVS)])\n",
        "\n",
        "# We implement frame-stacking via another custom wrapper by @brthor\n",
        "# It's a VecEnv wrapper, so it wraps the vec_env directly,\n",
        "# not in the builder lambda (make_env) like Monitor.\n",
        "# BatchedPytorchFrameStack returns a PytorchLazyFrames instance\n",
        "# when env.step() is called, instead of a numpy array. \n",
        "# The use of lazy frames avoids duplicating memory when frame-stacking.\n",
        "env = BatchedPytorchFrameStack(vec_env, k=4)\n",
        "\n",
        "# We create Doubly Ended Queue (deque) for fast append and pop (O(1))\n",
        "# Transition Buffer\n",
        "replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
        "# Episode Info Buffer\n",
        "ep_infos_buffer = deque([0.0], maxlen=100)\n",
        "\n",
        "episode_count = 0\n",
        "\n",
        "# Implement TensorBoard logging\n",
        "summary_writer = SummaryWriter(LOG_DIR)\n",
        "\n",
        "online_net = Network(env, device=device, double=USE_DOUBLE)\n",
        "target_net = Network(env, device=device, double=USE_DOUBLE)\n",
        "\n",
        "# Delegate networks to GPU (if device = 'cpu' this does nothing)\n",
        "online_net = online_net.to(device)\n",
        "target_net = online_net.to(device)\n",
        "\n",
        "# When at risk of OOM errors enable RELOAD_PARAMS to get back to last checkpoint\n",
        "if RELOAD_PARAMS:\n",
        "   online_net.load(SAVE_PATH)\n",
        "\n",
        "# We set the target net parameters equal to the online_net params\n",
        "# As specified in \"Human-level control through deep reinforcement learning\"\n",
        "target_net.load_state_dict(online_net.state_dict())\n",
        "\n",
        "# Create optimizer for gradient descent\n",
        "optimizer = torch.optim.Adam(online_net.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Initialize Replay Buffer\n",
        "obses = env.reset()\n",
        "\n",
        "# If do_init_action[i] is True then environment i executes action 1 (start game)\n",
        "do_init_action = [True for _ in range(N_ENVS)]\n",
        "\n",
        "######################\n",
        "# Replay Buffer Loop #\n",
        "######################\n",
        "\n",
        "for _ in range(MIN_REPLAY_SIZE):\n",
        "  # Select random actions\n",
        "  actions = [env.action_space.sample() for _ in range(N_ENVS)]\n",
        "\n",
        "  # If we are reloading parameters after a notebook disconnect or OOM error\n",
        "  # We build the replay set from the last network saved\n",
        "  if RELOAD_PARAMS:\n",
        "    # Epsilon decays linearly in time until reaching its final value\n",
        "    epsilon = np.interp(int(1e5) * N_ENVS, \n",
        "                        [0, EPSILON_DECAY],\n",
        "                        [EPSILON_START, EPSILON_END])\n",
        "    # Get the actions from the online network.\n",
        "    # If we are using frame-stacking with the custom wrapper\n",
        "    # we unwrap observations and stack frames before passing them to net.act(...).\n",
        "    # Epsilon-greedy policy is implemented in the net.act method\n",
        "    if isinstance(obses[0], PytorchLazyFrames):\n",
        "      act_obses = np.stack([o.get_frames() for o in obses])\n",
        "      actions = online_net.act(act_obses, EPSILON_START)\n",
        "    else:\n",
        "      actions = online_net.act(obses, EPSILON_START)\n",
        "\n",
        "  \n",
        "  # In the breakout game, we need to call action 1 each time a new game starts\n",
        "  # to release the projectile from the player's platform.\n",
        "  # We can help the agent by performing this action for them.\n",
        "  if FORCE_START:\n",
        "    actions = [1 if do_init_action[i] else a for i, a in enumerate(actions)]\n",
        "  \n",
        "  # We step the environment with the selected actions\n",
        "  new_obses, rewards, dones, infos = env.step(actions)\n",
        "  do_init_action = list(dones)\n",
        "\n",
        "  # We zip together all the info related to the current transition\n",
        "  # and iterate over the resulting collection.\n",
        "  # Experiences from all batches are grouped together in a common pool.\n",
        "  for obs, action, reward, done, new_obs, info in zip(obses, \n",
        "                                                actions, \n",
        "                                                rewards, \n",
        "                                                dones, \n",
        "                                                new_obses,\n",
        "                                                infos):\n",
        "    # We group all this info in a 'transition' tuple\n",
        "    # We put the tuple in the replay buffer to accumulate experience\n",
        "    # If an episode is done the VecEnv will env.reset() for us\n",
        "    transition = (obs, action, reward, done, new_obs)\n",
        "    replay_buffer.append(transition)\n",
        "\n",
        "  # We set the current observations as past obses for the new cycle\n",
        "  obses = new_obses\n",
        "\n",
        "######################\n",
        "# Main Training Loop #\n",
        "######################\n",
        "\n",
        "# After the random-actions loop we reset the environment and start training\n",
        "obses = env.reset()\n",
        "do_init_action = [True for _ in range(N_ENVS)]\n",
        "\n",
        "# Step the loop forward with the itertools.count() int generator\n",
        "for step in itertools.count():\n",
        "\n",
        "  # Epsilon decays linearly in time until reaching its final value\n",
        "  epsilon = np.interp(step * N_ENVS, \n",
        "                      [0, EPSILON_DECAY],\n",
        "                      [EPSILON_START, EPSILON_END])\n",
        "  \n",
        "  if isinstance(obses[0], PytorchLazyFrames):\n",
        "    act_obses = np.stack([o.get_frames() for o in obses])\n",
        "    actions = online_net.act(act_obses, epsilon)\n",
        "  else:\n",
        "    actions = online_net.act(obses, epsilon)\n",
        "  \n",
        "  if FORCE_START:\n",
        "    actions = [1 if do_init_action[i] else a for i, a in enumerate(actions)]\n",
        "    \n",
        "  # The training loop goes on as in the random-actions regime\n",
        "  new_obses, rewards, dones, infos = env.step(actions)\n",
        "  do_init_action = list(dones)\n",
        "\n",
        "  for obs, action, reward, done, new_obs, info in zip(obses, \n",
        "                                                actions, \n",
        "                                                rewards, \n",
        "                                                dones, \n",
        "                                                new_obses,\n",
        "                                                infos):\n",
        "    transition = (obs, action, reward, done, new_obs)\n",
        "    replay_buffer.append(transition)\n",
        "\n",
        "    # When an episode is done we append the episode info to the buffer.\n",
        "    # We pass the info to TensorBoard for out-of-the-box interactive graphs.\n",
        "    if done:\n",
        "      ep_infos_buffer.append(info['episode'])\n",
        "      episode_count += 1\n",
        "\n",
        "  obses = new_obses\n",
        "\n",
        "  # Start Gradient Step\n",
        "  transitions = random.sample(replay_buffer, BATCH_SIZE)\n",
        "  # Compute loss\n",
        "  loss = online_net.compute_loss(transitions, target_net)\n",
        "  # Gradient Descent\n",
        "  optimizer.zero_grad()\n",
        "  # Back-Propagation\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # Update Target Network with the online net weights\n",
        "  if step % TARGET_UPDATE_FREQ == 0:\n",
        "    target_net.load_state_dict(online_net.state_dict())\n",
        "\n",
        "  # Logging\n",
        "  if step % LOG_INTERVAL == 0:\n",
        "    if isinstance(ep_infos_buffer[0], dict):  \n",
        "      reward_mean = np.mean([e['r'] for e in ep_infos_buffer]) or 0\n",
        "      length_mean = np.mean([e['l'] for e in ep_infos_buffer]) or 0\n",
        "      # Log data to TensorBoard graphs\n",
        "      summary_writer.add_scalar('AvgRew', reward_mean, global_step=step)\n",
        "      summary_writer.add_scalar('AvgEpLen', length_mean, global_step=step)\n",
        "      summary_writer.add_scalar('Episodes', episode_count)\n",
        "    else:\n",
        "      reward_mean = 'N/A'\n",
        "      length_mean = 'N/A'\n",
        "\n",
        "    print()\n",
        "    print('Step', step)\n",
        "    print('Avg Reward', reward_mean)\n",
        "    print('Avg Episode Length', length_mean)\n",
        "    print('Episodes', episode_count)\n",
        "\n",
        "  # Save Network Parameters\n",
        "  if step % SAVE_INTERVAL == 0 and step != 0:\n",
        "    print('Saving...')\n",
        "    online_net.save(SAVE_PATH)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jU2zTbXbOoRa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ff258f2-fec5-4ce0-c528-c999192a5ed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/content/deep-q-learning-utils/baselines_wrappers/dummy_vec_env.py:25: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self.buf_dones = np.zeros((self.num_envs,), dtype=np.bool)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 0\n",
            "Avg Reward N/A\n",
            "Avg Episode Length N/A\n",
            "Episodes 1\n",
            "\n",
            "Step 1000\n",
            "Avg Reward 0.09\n",
            "Avg Episode Length 31.55\n",
            "Episodes 121\n",
            "\n",
            "Step 2000\n",
            "Avg Reward 0.25\n",
            "Avg Episode Length 36.7\n",
            "Episodes 233\n",
            "\n",
            "Step 3000\n",
            "Avg Reward 0.29\n",
            "Avg Episode Length 37.48\n",
            "Episodes 340\n",
            "\n",
            "Step 4000\n",
            "Avg Reward 0.26\n",
            "Avg Episode Length 38.1\n",
            "Episodes 445\n",
            "\n",
            "Step 5000\n",
            "Avg Reward 0.23\n",
            "Avg Episode Length 36.91\n",
            "Episodes 552\n",
            "\n",
            "Step 6000\n",
            "Avg Reward 0.38\n",
            "Avg Episode Length 41.27\n",
            "Episodes 649\n",
            "\n",
            "Step 7000\n",
            "Avg Reward 0.34\n",
            "Avg Episode Length 39.6\n",
            "Episodes 749\n",
            "\n",
            "Step 8000\n",
            "Avg Reward 0.39\n",
            "Avg Episode Length 41.73\n",
            "Episodes 846\n",
            "\n",
            "Step 9000\n",
            "Avg Reward 0.44\n",
            "Avg Episode Length 44.39\n",
            "Episodes 935\n",
            "\n",
            "Step 10000\n",
            "Avg Reward 0.28\n",
            "Avg Episode Length 37.88\n",
            "Episodes 1038\n",
            "Saving...\n",
            "\n",
            "Step 11000\n",
            "Avg Reward 0.27\n",
            "Avg Episode Length 37.33\n",
            "Episodes 1148\n",
            "\n",
            "Step 12000\n",
            "Avg Reward 0.27\n",
            "Avg Episode Length 37.03\n",
            "Episodes 1254\n",
            "\n",
            "Step 13000\n",
            "Avg Reward 0.2\n",
            "Avg Episode Length 33.58\n",
            "Episodes 1371\n",
            "\n",
            "Step 14000\n",
            "Avg Reward 0.34\n",
            "Avg Episode Length 39.27\n",
            "Episodes 1475\n",
            "\n",
            "Step 15000\n",
            "Avg Reward 0.35\n",
            "Avg Episode Length 40.74\n",
            "Episodes 1569\n",
            "\n",
            "Step 16000\n",
            "Avg Reward 0.38\n",
            "Avg Episode Length 40.32\n",
            "Episodes 1667\n",
            "\n",
            "Step 17000\n",
            "Avg Reward 0.47\n",
            "Avg Episode Length 46.3\n",
            "Episodes 1751\n",
            "\n",
            "Step 18000\n",
            "Avg Reward 0.33\n",
            "Avg Episode Length 38.33\n",
            "Episodes 1854\n",
            "\n",
            "Step 19000\n",
            "Avg Reward 0.35\n",
            "Avg Episode Length 39.2\n",
            "Episodes 1958\n",
            "\n",
            "Step 20000\n",
            "Avg Reward 0.42\n",
            "Avg Episode Length 42.1\n",
            "Episodes 2048\n",
            "Saving...\n",
            "\n",
            "Step 21000\n",
            "Avg Reward 0.31\n",
            "Avg Episode Length 38.18\n",
            "Episodes 2154\n",
            "\n",
            "Step 22000\n",
            "Avg Reward 0.24\n",
            "Avg Episode Length 35.88\n",
            "Episodes 2265\n",
            "\n",
            "Step 23000\n",
            "Avg Reward 0.3\n",
            "Avg Episode Length 38.58\n",
            "Episodes 2366\n",
            "\n",
            "Step 24000\n",
            "Avg Reward 0.41\n",
            "Avg Episode Length 41.62\n",
            "Episodes 2463\n",
            "\n",
            "Step 25000\n",
            "Avg Reward 0.44\n",
            "Avg Episode Length 43.74\n",
            "Episodes 2555\n",
            "\n",
            "Step 26000\n",
            "Avg Reward 0.43\n",
            "Avg Episode Length 43.25\n",
            "Episodes 2650\n",
            "\n",
            "Step 27000\n",
            "Avg Reward 0.43\n",
            "Avg Episode Length 42.92\n",
            "Episodes 2741\n",
            "\n",
            "Step 28000\n",
            "Avg Reward 0.48\n",
            "Avg Episode Length 46.77\n",
            "Episodes 2829\n",
            "\n",
            "Step 29000\n",
            "Avg Reward 0.45\n",
            "Avg Episode Length 43.88\n",
            "Episodes 2923\n",
            "\n",
            "Step 30000\n",
            "Avg Reward 0.36\n",
            "Avg Episode Length 40.18\n",
            "Episodes 3020\n",
            "Saving...\n",
            "\n",
            "Step 31000\n",
            "Avg Reward 0.41\n",
            "Avg Episode Length 42.25\n",
            "Episodes 3113\n",
            "\n",
            "Step 32000\n",
            "Avg Reward 0.57\n",
            "Avg Episode Length 48.27\n",
            "Episodes 3198\n",
            "\n",
            "Step 33000\n",
            "Avg Reward 0.4\n",
            "Avg Episode Length 43.12\n",
            "Episodes 3293\n",
            "\n",
            "Step 34000\n",
            "Avg Reward 0.43\n",
            "Avg Episode Length 42.59\n",
            "Episodes 3383\n",
            "\n",
            "Step 35000\n",
            "Avg Reward 0.45\n",
            "Avg Episode Length 43.08\n",
            "Episodes 3475\n",
            "\n",
            "Step 36000\n",
            "Avg Reward 0.44\n",
            "Avg Episode Length 42.26\n",
            "Episodes 3573\n",
            "\n",
            "Step 37000\n",
            "Avg Reward 0.43\n",
            "Avg Episode Length 43.1\n",
            "Episodes 3666\n",
            "\n",
            "Step 38000\n",
            "Avg Reward 0.57\n",
            "Avg Episode Length 47.37\n",
            "Episodes 3750\n",
            "\n",
            "Step 39000\n",
            "Avg Reward 0.46\n",
            "Avg Episode Length 45.39\n",
            "Episodes 3836\n",
            "\n",
            "Step 40000\n",
            "Avg Reward 0.58\n",
            "Avg Episode Length 48.8\n",
            "Episodes 3913\n",
            "Saving...\n",
            "\n",
            "Step 41000\n",
            "Avg Reward 0.62\n",
            "Avg Episode Length 51.36\n",
            "Episodes 3993\n",
            "\n",
            "Step 42000\n",
            "Avg Reward 0.58\n",
            "Avg Episode Length 50.0\n",
            "Episodes 4067\n",
            "\n",
            "Step 43000\n",
            "Avg Reward 0.55\n",
            "Avg Episode Length 48.76\n",
            "Episodes 4152\n",
            "\n",
            "Step 44000\n",
            "Avg Reward 0.7\n",
            "Avg Episode Length 53.23\n",
            "Episodes 4230\n",
            "\n",
            "Step 45000\n",
            "Avg Reward 0.64\n",
            "Avg Episode Length 50.72\n",
            "Episodes 4314\n",
            "\n",
            "Step 46000\n",
            "Avg Reward 0.61\n",
            "Avg Episode Length 50.02\n",
            "Episodes 4397\n",
            "\n",
            "Step 47000\n",
            "Avg Reward 0.76\n",
            "Avg Episode Length 57.51\n",
            "Episodes 4462\n",
            "\n",
            "Step 48000\n",
            "Avg Reward 0.65\n",
            "Avg Episode Length 51.94\n",
            "Episodes 4537\n",
            "\n",
            "Step 49000\n",
            "Avg Reward 0.55\n",
            "Avg Episode Length 45.95\n",
            "Episodes 4627\n",
            "\n",
            "Step 50000\n",
            "Avg Reward 0.57\n",
            "Avg Episode Length 46.99\n",
            "Episodes 4713\n",
            "Saving...\n",
            "\n",
            "Step 51000\n",
            "Avg Reward 0.56\n",
            "Avg Episode Length 46.81\n",
            "Episodes 4801\n",
            "\n",
            "Step 52000\n",
            "Avg Reward 0.6\n",
            "Avg Episode Length 48.65\n",
            "Episodes 4885\n",
            "\n",
            "Step 53000\n",
            "Avg Reward 0.65\n",
            "Avg Episode Length 50.81\n",
            "Episodes 4966\n",
            "\n",
            "Step 54000\n",
            "Avg Reward 0.51\n",
            "Avg Episode Length 44.43\n",
            "Episodes 5060\n",
            "\n",
            "Step 55000\n",
            "Avg Reward 0.6\n",
            "Avg Episode Length 48.32\n",
            "Episodes 5147\n",
            "\n",
            "Step 56000\n",
            "Avg Reward 0.74\n",
            "Avg Episode Length 54.1\n",
            "Episodes 5218\n",
            "\n",
            "Step 57000\n",
            "Avg Reward 0.71\n",
            "Avg Episode Length 52.53\n",
            "Episodes 5298\n",
            "\n",
            "Step 58000\n",
            "Avg Reward 0.51\n",
            "Avg Episode Length 45.34\n",
            "Episodes 5385\n",
            "\n",
            "Step 59000\n",
            "Avg Reward 0.66\n",
            "Avg Episode Length 50.85\n",
            "Episodes 5457\n",
            "\n",
            "Step 60000\n",
            "Avg Reward 0.55\n",
            "Avg Episode Length 47.79\n",
            "Episodes 5539\n",
            "Saving...\n",
            "\n",
            "Step 61000\n",
            "Avg Reward 0.63\n",
            "Avg Episode Length 51.15\n",
            "Episodes 5613\n",
            "\n",
            "Step 62000\n",
            "Avg Reward 0.61\n",
            "Avg Episode Length 49.37\n",
            "Episodes 5693\n",
            "\n",
            "Step 63000\n",
            "Avg Reward 0.77\n",
            "Avg Episode Length 56.02\n",
            "Episodes 5771\n",
            "\n",
            "Step 64000\n",
            "Avg Reward 0.81\n",
            "Avg Episode Length 57.32\n",
            "Episodes 5838\n",
            "\n",
            "Step 65000\n",
            "Avg Reward 0.77\n",
            "Avg Episode Length 56.35\n",
            "Episodes 5911\n",
            "\n",
            "Step 66000\n",
            "Avg Reward 0.78\n",
            "Avg Episode Length 55.35\n",
            "Episodes 5980\n",
            "\n",
            "Step 67000\n",
            "Avg Reward 0.82\n",
            "Avg Episode Length 56.91\n",
            "Episodes 6052\n",
            "\n",
            "Step 68000\n",
            "Avg Reward 0.7\n",
            "Avg Episode Length 52.2\n",
            "Episodes 6132\n",
            "\n",
            "Step 69000\n",
            "Avg Reward 0.7\n",
            "Avg Episode Length 52.63\n",
            "Episodes 6205\n",
            "\n",
            "Step 70000\n",
            "Avg Reward 0.78\n",
            "Avg Episode Length 54.87\n",
            "Episodes 6278\n",
            "Saving...\n",
            "\n",
            "Step 71000\n",
            "Avg Reward 0.71\n",
            "Avg Episode Length 51.78\n",
            "Episodes 6352\n",
            "\n",
            "Step 72000\n",
            "Avg Reward 0.78\n",
            "Avg Episode Length 53.34\n",
            "Episodes 6424\n",
            "\n",
            "Step 73000\n",
            "Avg Reward 0.93\n",
            "Avg Episode Length 61.49\n",
            "Episodes 6482\n",
            "\n",
            "Step 74000\n",
            "Avg Reward 0.71\n",
            "Avg Episode Length 53.39\n",
            "Episodes 6556\n",
            "\n",
            "Step 75000\n",
            "Avg Reward 0.72\n",
            "Avg Episode Length 52.56\n",
            "Episodes 6634\n",
            "\n",
            "Step 76000\n",
            "Avg Reward 0.81\n",
            "Avg Episode Length 56.41\n",
            "Episodes 6702\n",
            "\n",
            "Step 77000\n",
            "Avg Reward 0.79\n",
            "Avg Episode Length 55.74\n",
            "Episodes 6775\n",
            "\n",
            "Step 78000\n",
            "Avg Reward 0.82\n",
            "Avg Episode Length 57.09\n",
            "Episodes 6844\n",
            "\n",
            "Step 79000\n",
            "Avg Reward 0.78\n",
            "Avg Episode Length 55.69\n",
            "Episodes 6915\n",
            "\n",
            "Step 80000\n",
            "Avg Reward 0.83\n",
            "Avg Episode Length 58.33\n",
            "Episodes 6987\n",
            "Saving...\n",
            "\n",
            "Step 81000\n",
            "Avg Reward 0.77\n",
            "Avg Episode Length 55.0\n",
            "Episodes 7062\n",
            "\n",
            "Step 82000\n",
            "Avg Reward 0.86\n",
            "Avg Episode Length 58.54\n",
            "Episodes 7126\n",
            "\n",
            "Step 83000\n",
            "Avg Reward 0.91\n",
            "Avg Episode Length 60.97\n",
            "Episodes 7188\n",
            "\n",
            "Step 84000\n",
            "Avg Reward 0.81\n",
            "Avg Episode Length 57.11\n",
            "Episodes 7257\n",
            "\n",
            "Step 85000\n",
            "Avg Reward 0.83\n",
            "Avg Episode Length 58.0\n",
            "Episodes 7323\n",
            "\n",
            "Step 86000\n",
            "Avg Reward 0.83\n",
            "Avg Episode Length 57.55\n",
            "Episodes 7396\n",
            "\n",
            "Step 87000\n",
            "Avg Reward 0.85\n",
            "Avg Episode Length 58.33\n",
            "Episodes 7464\n",
            "\n",
            "Step 88000\n",
            "Avg Reward 1.05\n",
            "Avg Episode Length 66.01\n",
            "Episodes 7526\n",
            "\n",
            "Step 89000\n",
            "Avg Reward 0.98\n",
            "Avg Episode Length 62.52\n",
            "Episodes 7590\n",
            "\n",
            "Step 90000\n",
            "Avg Reward 1.04\n",
            "Avg Episode Length 64.76\n",
            "Episodes 7651\n",
            "Saving...\n",
            "\n",
            "Step 91000\n",
            "Avg Reward 1.15\n",
            "Avg Episode Length 68.78\n",
            "Episodes 7713\n",
            "\n",
            "Step 92000\n",
            "Avg Reward 0.94\n",
            "Avg Episode Length 61.62\n",
            "Episodes 7774\n",
            "\n",
            "Step 93000\n",
            "Avg Reward 1.1\n",
            "Avg Episode Length 68.05\n",
            "Episodes 7832\n",
            "\n",
            "Step 94000\n",
            "Avg Reward 1.14\n",
            "Avg Episode Length 70.4\n",
            "Episodes 7887\n",
            "\n",
            "Step 95000\n",
            "Avg Reward 1.04\n",
            "Avg Episode Length 65.8\n",
            "Episodes 7955\n",
            "\n",
            "Step 96000\n",
            "Avg Reward 0.96\n",
            "Avg Episode Length 60.63\n",
            "Episodes 8021\n",
            "\n",
            "Step 97000\n",
            "Avg Reward 0.79\n",
            "Avg Episode Length 56.41\n",
            "Episodes 8090\n",
            "\n",
            "Step 98000\n",
            "Avg Reward 1.06\n",
            "Avg Episode Length 68.5\n",
            "Episodes 8143\n",
            "\n",
            "Step 99000\n",
            "Avg Reward 1.15\n",
            "Avg Episode Length 70.21\n",
            "Episodes 8201\n",
            "\n",
            "Step 100000\n",
            "Avg Reward 1.14\n",
            "Avg Episode Length 70.44\n",
            "Episodes 8260\n",
            "Saving...\n",
            "\n",
            "Step 101000\n",
            "Avg Reward 0.94\n",
            "Avg Episode Length 60.99\n",
            "Episodes 8321\n",
            "\n",
            "Step 102000\n",
            "Avg Reward 1.09\n",
            "Avg Episode Length 66.02\n",
            "Episodes 8381\n",
            "\n",
            "Step 103000\n",
            "Avg Reward 0.91\n",
            "Avg Episode Length 59.95\n",
            "Episodes 8451\n",
            "\n",
            "Step 104000\n",
            "Avg Reward 1.01\n",
            "Avg Episode Length 63.5\n",
            "Episodes 8513\n",
            "\n",
            "Step 105000\n",
            "Avg Reward 1.1\n",
            "Avg Episode Length 67.78\n",
            "Episodes 8570\n",
            "\n",
            "Step 106000\n",
            "Avg Reward 1.24\n",
            "Avg Episode Length 74.52\n",
            "Episodes 8621\n",
            "\n",
            "Step 107000\n",
            "Avg Reward 1.33\n",
            "Avg Episode Length 77.07\n",
            "Episodes 8671\n",
            "\n",
            "Step 108000\n",
            "Avg Reward 1.21\n",
            "Avg Episode Length 71.69\n",
            "Episodes 8727\n",
            "\n",
            "Step 109000\n",
            "Avg Reward 1.18\n",
            "Avg Episode Length 70.04\n",
            "Episodes 8789\n",
            "\n",
            "Step 110000\n",
            "Avg Reward 1.21\n",
            "Avg Episode Length 71.56\n",
            "Episodes 8842\n",
            "Saving...\n",
            "\n",
            "Step 111000\n",
            "Avg Reward 1.2\n",
            "Avg Episode Length 71.09\n",
            "Episodes 8900\n",
            "\n",
            "Step 112000\n",
            "Avg Reward 1.22\n",
            "Avg Episode Length 71.32\n",
            "Episodes 8955\n",
            "\n",
            "Step 113000\n",
            "Avg Reward 1.23\n",
            "Avg Episode Length 74.24\n",
            "Episodes 9009\n",
            "\n",
            "Step 114000\n",
            "Avg Reward 1.3\n",
            "Avg Episode Length 77.53\n",
            "Episodes 9062\n",
            "\n",
            "Step 115000\n",
            "Avg Reward 1.29\n",
            "Avg Episode Length 74.45\n",
            "Episodes 9114\n",
            "\n",
            "Step 116000\n",
            "Avg Reward 1.3\n",
            "Avg Episode Length 74.24\n",
            "Episodes 9171\n",
            "\n",
            "Step 117000\n",
            "Avg Reward 1.13\n",
            "Avg Episode Length 66.75\n",
            "Episodes 9228\n",
            "\n",
            "Step 118000\n",
            "Avg Reward 1.44\n",
            "Avg Episode Length 78.85\n",
            "Episodes 9272\n",
            "\n",
            "Step 119000\n",
            "Avg Reward 1.58\n",
            "Avg Episode Length 86.17\n",
            "Episodes 9319\n",
            "\n",
            "Step 120000\n",
            "Avg Reward 1.31\n",
            "Avg Episode Length 76.6\n",
            "Episodes 9378\n",
            "Saving...\n",
            "\n",
            "Step 121000\n",
            "Avg Reward 1.39\n",
            "Avg Episode Length 78.39\n",
            "Episodes 9423\n",
            "\n",
            "Step 122000\n",
            "Avg Reward 1.5\n",
            "Avg Episode Length 82.49\n",
            "Episodes 9471\n",
            "\n",
            "Step 123000\n",
            "Avg Reward 1.48\n",
            "Avg Episode Length 82.66\n",
            "Episodes 9519\n",
            "\n",
            "Step 124000\n",
            "Avg Reward 1.43\n",
            "Avg Episode Length 81.55\n",
            "Episodes 9568\n",
            "\n",
            "Step 125000\n",
            "Avg Reward 1.6\n",
            "Avg Episode Length 88.09\n",
            "Episodes 9607\n",
            "\n",
            "Step 126000\n",
            "Avg Reward 1.68\n",
            "Avg Episode Length 92.21\n",
            "Episodes 9658\n",
            "\n",
            "Step 127000\n",
            "Avg Reward 1.4\n",
            "Avg Episode Length 79.29\n",
            "Episodes 9708\n",
            "\n",
            "Step 128000\n",
            "Avg Reward 1.2\n",
            "Avg Episode Length 72.75\n",
            "Episodes 9766\n",
            "\n",
            "Step 129000\n",
            "Avg Reward 1.17\n",
            "Avg Episode Length 72.25\n",
            "Episodes 9817\n",
            "\n",
            "Step 130000\n",
            "Avg Reward 1.5\n",
            "Avg Episode Length 84.42\n",
            "Episodes 9861\n",
            "Saving...\n",
            "\n",
            "Step 131000\n",
            "Avg Reward 1.55\n",
            "Avg Episode Length 85.84\n",
            "Episodes 9910\n",
            "\n",
            "Step 132000\n",
            "Avg Reward 1.41\n",
            "Avg Episode Length 80.09\n",
            "Episodes 9963\n",
            "\n",
            "Step 133000\n",
            "Avg Reward 1.31\n",
            "Avg Episode Length 75.01\n",
            "Episodes 10014\n",
            "\n",
            "Step 134000\n",
            "Avg Reward 1.35\n",
            "Avg Episode Length 75.53\n",
            "Episodes 10069\n",
            "\n",
            "Step 135000\n",
            "Avg Reward 1.29\n",
            "Avg Episode Length 75.3\n",
            "Episodes 10120\n",
            "\n",
            "Step 136000\n",
            "Avg Reward 1.33\n",
            "Avg Episode Length 74.53\n",
            "Episodes 10174\n",
            "\n",
            "Step 137000\n",
            "Avg Reward 1.47\n",
            "Avg Episode Length 78.5\n",
            "Episodes 10222\n",
            "\n",
            "Step 138000\n",
            "Avg Reward 1.53\n",
            "Avg Episode Length 83.39\n",
            "Episodes 10264\n",
            "\n",
            "Step 139000\n",
            "Avg Reward 1.74\n",
            "Avg Episode Length 91.28\n",
            "Episodes 10310\n",
            "\n",
            "Step 140000\n",
            "Avg Reward 1.7\n",
            "Avg Episode Length 88.82\n",
            "Episodes 10354\n",
            "Saving...\n",
            "\n",
            "Step 141000\n",
            "Avg Reward 1.79\n",
            "Avg Episode Length 92.14\n",
            "Episodes 10399\n",
            "\n",
            "Step 142000\n",
            "Avg Reward 1.59\n",
            "Avg Episode Length 85.29\n",
            "Episodes 10449\n",
            "\n",
            "Step 143000\n",
            "Avg Reward 1.68\n",
            "Avg Episode Length 88.08\n",
            "Episodes 10489\n",
            "\n",
            "Step 144000\n",
            "Avg Reward 1.81\n",
            "Avg Episode Length 92.74\n",
            "Episodes 10535\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graphs with TensorBoard"
      ],
      "metadata": {
        "id": "O19jDGbX4I9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir $LOG_DIR"
      ],
      "metadata": {
        "id": "Ze_oh1_u2yIW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}