{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1LmIgiaTlfQ7eRmbBIg2YjU5XYm7pRlJq",
      "authorship_tag": "ABX9TyNCVTbZldduxanD06hKXDaA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matteo4diani/deep-q-learning/blob/main/deepqlearning_atari.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Setup\n",
        "\n"
      ],
      "metadata": {
        "id": "Rvnw0S_yNDGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installs for:\n",
        "# ML framework (pytorch)\n",
        "# Serialization (msgpack)\n",
        "# Atari env (gym)\n",
        "# Tensorflow introspection and visualization (tensorboard)\n",
        "!pip install torch gym\n",
        "!pip install autorom[accept-rom-license]\n",
        "!sudo apt-get install zlib1g-dev cmake\n",
        "!pip install 'msgpack==1.0.2' gym[atari] tensorboard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0cDzQB6OE0W",
        "outputId": "d98c1b85-def2-463b-bf96-34f18e0cb22a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.25.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym) (3.8.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting autorom[accept-rom-license]\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]) (7.1.2)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]) (5.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]) (4.64.1)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->autorom[accept-rom-license]) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]) (2022.6.15)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=b07667b0a9c311a89434e93680885dc77d768dda0266ba7e9e3d3a10d707b6ca\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 autorom-0.4.2\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.2).\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2.2).\n",
            "zlib1g-dev set to manually installed.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 32 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting msgpack==1.0.2\n",
            "  Downloading msgpack-1.0.2-cp37-cp37m-manylinux1_x86_64.whl (273 kB)\n",
            "\u001b[K     |████████████████████████████████| 273 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym[atari] in /usr/local/lib/python3.7/dist-packages (0.25.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.4.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.37.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.21.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.2.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.48.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (2.23.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (2022.6.15)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (0.0.8)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 65.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[atari]) (5.9.0)\n",
            "Installing collected packages: ale-py, msgpack\n",
            "  Attempting uninstall: msgpack\n",
            "    Found existing installation: msgpack 1.0.4\n",
            "    Uninstalling msgpack-1.0.4:\n",
            "      Successfully uninstalled msgpack-1.0.4\n",
            "Successfully installed ale-py-0.7.5 msgpack-1.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-9EsR3Uqmdr3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2793d27-6af2-4160-c691-cd02e5963aa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'deep-q-learning-utils'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 30 (delta 6), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (30/30), done.\n"
          ]
        }
      ],
      "source": [
        "# Clone PyTorch/StableBaselines wrappers\n",
        "!git clone https://github.com/matteo4diani/deep-q-learning-utils.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Put the downloaded repo on path so we can import custom libraries into Colab\n",
        "import sys\n",
        "sys.path.insert(0,'/content/deep-q-learning-utils')"
      ],
      "metadata": {
        "id": "6Rvzb2eNJLY_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove persistence directories\n",
        "import shutil\n",
        "shutil.rmtree('/content/gdrive/MyDrive/deep-q-learning-atari/checkpoints', ignore_errors=True)\n",
        "shutil.rmtree('/content/gdrive/MyDrive/deep-q-learning-atari/tensorboard', ignore_errors=True)"
      ],
      "metadata": {
        "id": "5LNcaVIWOzXi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Imports"
      ],
      "metadata": {
        "id": "MLEXZszENvvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch\n",
        "import gym\n",
        "from collections import deque\n",
        "import itertools\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "from pathlib import Path\n",
        "\n",
        "from pytorch_wrappers import make_atari_deepmind, BatchedPytorchFrameStack, PytorchLazyFrames\n",
        "from baselines_wrappers import Monitor, DummyVecEnv, SubprocVecEnv\n",
        "\n",
        "import msgpack\n",
        "from msgpack_numpy import patch as msgpack_numpy_patch\n",
        "msgpack_numpy_patch()\n",
        "# Load TensorBoard extension\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "QMeJdJG3JLfz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constants"
      ],
      "metadata": {
        "id": "GcXfQjQlN2dA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################################\n",
        "# Model Constants (taken from:  #\n",
        "# \"Human-level control through  #\n",
        "# deep reinforcement learning\") #\n",
        "#################################\n",
        "\n",
        "# Discount rate\n",
        "GAMMA = 0.99\n",
        "# How many transitions to sample from\n",
        "BATCH_SIZE = 32\n",
        "# How many transitions we're gonna store before overwrite\n",
        "BUFFER_SIZE = int(1e6)\n",
        "# How many transitions to accumulate before we start the actual training\n",
        "MIN_REPLAY_SIZE = 50000\n",
        "# Starting value of epsilon (probability of taking random action)\n",
        "EPSILON_START = 1.0\n",
        "# Final value of epsilon\n",
        "EPSILON_END = 0.1\n",
        "# Number of steps taken for EPSILON_START to become EPSILON_END\n",
        "EPSILON_DECAY = int(1e6)\n",
        "# Number of batch elements (environments created)\n",
        "N_ENVS = 4\n",
        "# Periodicity for target updates with the online values\n",
        "TARGET_UPDATE_FREQ = 10000 // N_ENVS\n",
        "# Learning Rate\n",
        "LEARNING_RATE = 5e-5\n",
        "# If True force taking action 1 at the start of each round to initiate gameplay\n",
        "FORCE_START = True\n",
        "#####################\n",
        "# Utility Constants #\n",
        "#####################\n",
        "SAVE_PATHS = {True:  '/content/gdrive/MyDrive/deep-q-learning-atari/checkpoints/atari_model.pack', \n",
        "              False: 'checkpoints/atari_model.pack'}\n",
        "LOG_DIRS = {True: '/content/gdrive/MyDrive/deep-q-learning-atari/tensorboard/atari_model',\n",
        "            False: 'tensorboard/atari_model'}\n",
        "# Use your personal Google Drive for parameter serialization and logs\n",
        "USE_DRIVE = True\n",
        "# Reload parameters from disk/drive\n",
        "RELOAD_PARAMS = False\n",
        "# Path for network parameters serialization\n",
        "SAVE_PATH = SAVE_PATHS[USE_DRIVE]\n",
        "SAVE_INTERVAL = 10000\n",
        "# Path for TensorBoard logging\n",
        "LOG_DIR = LOG_DIRS[USE_DRIVE]\n",
        "LOG_INTERVAL = 1000\n",
        "\n"
      ],
      "metadata": {
        "id": "ACMeYFuxN2qT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount google drive so we can store data between runs\n",
        "from google.colab import drive\n",
        "if USE_DRIVE:\n",
        "  drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSBlsAg5ihWu",
        "outputId": "48977789-fc93-4f8e-cb44-7c154bdb92bb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network Definition"
      ],
      "metadata": {
        "id": "vaTIZATCN3AF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.activation import ReLU\n",
        "\n",
        "def nature_cnn(observation_space, depths=(32, 64, 64), final_layer=512):\n",
        "  \"\"\"\n",
        "  CNN architecture as defined in 'Human-level Control through \n",
        "  deep reinforcement learning'\n",
        "  \"\"\"\n",
        "  # Get the number of input channels\n",
        "  n_input_channels = observation_space.shape[0]\n",
        "\n",
        "  cnn = nn.Sequential(\n",
        "      nn.Conv2d(n_input_channels, depths[0], kernel_size=8, stride=4),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(depths[0], depths[1], kernel_size=4, stride=2),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(depths[1], depths[2], kernel_size=3, stride=1),\n",
        "      nn.ReLU(),\n",
        "      nn.Flatten())\n",
        "  # Compute shape by doing one forward pass through the cnn\n",
        "  # and looking at the output shape of the tensor\n",
        "  with torch.no_grad():\n",
        "    # We are not passing this tensor to the gpu:\n",
        "    # Our NNs will still be on the CPU when nature_cnn(...) is called.\n",
        "    n_flatten = cnn(torch.as_tensor(observation_space.sample()[None]).float()).shape[1]\n",
        "    out = nn.Sequential(cnn, nn.Linear(n_flatten, final_layer), nn.ReLU())\n",
        "  \n",
        "  return out\n",
        "\n",
        "# Class representing the neural network, implements PyTorch nn.Module interface\n",
        "class Network(nn.Module):\n",
        "  def __init__(self, env, device):\n",
        "    super().__init__()\n",
        "    # Enable GPU support with explicit tensor/model allocation\n",
        "    self.device = device\n",
        "    # Number of actions available to the agent\n",
        "    self.num_actions = env.action_space.n\n",
        "    # Get Nature CNN instance\n",
        "    conv_net = nature_cnn(env.observation_space)\n",
        "    # Create network stacking the Nature CNN and a last layer \n",
        "    # dependent on the game environment \n",
        "    # (different num_actions, not knowable a-priori)\n",
        "    self.net = nn.Sequential(conv_net, nn.Linear(512, self.num_actions))\n",
        "  # Forward function is part of the interface for nn.Module\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "  def act(self, obses, epsilon):\n",
        "    # Convert observations to PyTorch tensor\n",
        "    obses_tensor = torch.as_tensor(obses, \n",
        "                                   dtype=torch.float32, \n",
        "                                   device=self.device)\n",
        "    \n",
        "    # PyTorch already expects a batch of samples so we pass the tensor as-is\n",
        "    # and we get a prediction from the Q-Network\n",
        "    q_values = self(obses_tensor)\n",
        "\n",
        "    # Get argmaxes of actions with best q\n",
        "    max_q_indices = torch.argmax(q_values, dim=1)\n",
        "    # Cast tensor into list of ints\n",
        "    actions = max_q_indices.detach().tolist()\n",
        "\n",
        "    # Implement epsilon-greedy policy.\n",
        "    # We get P(random action) = epsilon by P(randint(0,1) <= epsilon) = epsilon\n",
        "    for i in range(len(actions)):\n",
        "      rnd_sample = random.random()\n",
        "      if rnd_sample <= epsilon:\n",
        "        actions[i] = random.randint(0, self.num_actions - 1)\n",
        "    \n",
        "    return actions\n",
        "\n",
        "  def compute_loss(self, transitions, target_net):\n",
        "    # Comb data and turn to numpy array for faster runs\n",
        "    obses = [t[0] for t in transitions]\n",
        "    actions = np.asarray([t[1] for t in transitions])\n",
        "    rewards = np.asarray([t[2] for t in transitions])\n",
        "    dones = np.asarray([t[3] for t in transitions])\n",
        "    new_obses = [t[4] for t in transitions]\n",
        "    \n",
        "    # If using frame-stacking use helper get_frames() to get numpy compliant obj\n",
        "    if isinstance(obses[0], PytorchLazyFrames):\n",
        "      obses = np.stack([o.get_frames() for o in obses])\n",
        "      new_obses = np.stack([o.get_frames() for o in new_obses])\n",
        "    else:\n",
        "      obses = np.asarray(obses)\n",
        "      new_obses = np.asarray(new_obses)\n",
        "\n",
        "    # Turn to PyTorch tensor\n",
        "    obses_tensor = torch.as_tensor(obses, \n",
        "                                   dtype=torch.float32, \n",
        "                                   device=self.device)\n",
        "    # We unsqueeze(-1) to wrap each action/rew/... in an additional dimension\n",
        "    actions_tensor = torch.as_tensor(actions,\n",
        "                                     dtype=torch.int64,\n",
        "                                     device=self.device).unsqueeze(-1)\n",
        "    rewards_tensor = torch.as_tensor(rewards, \n",
        "                                     dtype=torch.float32,\n",
        "                                     device=self.device).unsqueeze(-1)\n",
        "    dones_tensor = torch.as_tensor(dones, \n",
        "                                   dtype=torch.float32,\n",
        "                                   device=self.device).unsqueeze(-1)\n",
        "    new_obses_tensor = torch.as_tensor(new_obses, \n",
        "                                       dtype=torch.float32,\n",
        "                                       device=self.device)\n",
        "\n",
        "    # Compute targets for loss function\n",
        "    # We use the target net to predict target q-values for new obses\n",
        "    # For each new observation we have a set of q-values\n",
        "    # We need to condense this set to the one highest q-value per observation\n",
        "    target_q_values = target_net(new_obses_tensor)\n",
        "    max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]\n",
        "    # (N.B. pytorch tensors .max() return argmax at index 1)\n",
        "\n",
        "\n",
        "    # Compute r + gamma*max(Q) (\"if done -> r\" is obtained via \"1 - dones_tensor\")\n",
        "    targets = rewards_tensor + GAMMA * (1 - dones_tensor) * max_target_q_values\n",
        "\n",
        "    # Compute Loss\n",
        "    q_values = self(obses_tensor)\n",
        "\n",
        "    # Get q-values for the actions we took\n",
        "    action_q_values = torch.gather(input=q_values, dim=1, index=actions_tensor)\n",
        "\n",
        "    # Compute l1 loss\n",
        "    loss = nn.functional.smooth_l1_loss(action_q_values, targets)\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def save(self, save_path):\n",
        "    \"\"\"Serialize network parameters to disk or Google Drive\"\"\"\n",
        "\n",
        "    # We call .cpu() to transfer the tensor\n",
        "    # from the gpu when converting to np array\n",
        "    params = {k: t.detach().cpu().numpy() for k, t in self.state_dict().items()}\n",
        "    # Serialize network parameter dictionary with msgpack\n",
        "    params_data = msgpack.dumps(params)\n",
        "\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "    with open(save_path, 'wb') as f:\n",
        "      f.write(params_data)\n",
        "\n",
        "  def load(self, load_path):\n",
        "    \"\"\"Load network parameters from disk or Google Drive\"\"\"\n",
        "    if not os.path.exists(load_path):\n",
        "      raise FileNotFoundError(load_path)\n",
        "\n",
        "    with open(load_path, 'rb') as f:\n",
        "      params_numpy = msgpack.load(f)\n",
        "      # Convert to PyTorch tensors and load into network\n",
        "      params = {k: torch.as_tensor(v, device=self.device) for k,v in params_numpy.items()}\n",
        "      self.load_state_dict(params)"
      ],
      "metadata": {
        "id": "wvFAh-pXN3S4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "J4QiJ5V4OoGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable GPU support\n",
        "device = torch.device('cuda:0') if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Load Breakout environment\n",
        "# We use a custom wrapper made by @brthor\n",
        "# The wrapper applies all the preprocessing steps described in\n",
        "# \"Human-level control through deep reinforcement learning\"\n",
        "# before the agent sees the observation.\n",
        "# It also transforms [Height, Width, Channel] -> [C, H, W] (PyTorch format)\n",
        "# Where H, W identify the pixel and channel is R, G or B.\n",
        "# We wrap make_atari_deepmind in a Monitor object that enriches the info\n",
        "# returned by env.step()\n",
        "make_env = lambda: Monitor(make_atari_deepmind('BreakoutNoFrameskip-v4',\n",
        "                                               scale_values=True), \n",
        "                           allow_early_resets=True)\n",
        "\n",
        "# Double configuration for VecEnv: sequential (dummy) and parallel (subproc)\n",
        "vec_env = DummyVecEnv([make_env for _ in range(N_ENVS)])\n",
        "#env = SubprocVecEnv([make_env for _ in range(N_ENVS)])\n",
        "\n",
        "# We implement frame-stacking via another custom wrapper by @brthor\n",
        "# It's a VecEnv wrapper, so it wraps the vec_env directly,\n",
        "# not in the builder lambda (make_env) like Monitor.\n",
        "# BatchedPytorchFrameStack returns a PytorchLazyFrames instance\n",
        "# when env.step() is called, instead of a numpy array. \n",
        "# The use of lazy frames avoids duplicating memory when frame-stacking.\n",
        "env = BatchedPytorchFrameStack(vec_env, k=4)\n",
        "\n",
        "# We create Doubly Ended Queue (deque) for fast append and pop (O(1))\n",
        "# Transition Buffer\n",
        "replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
        "# Episode Info Buffer\n",
        "ep_infos_buffer = deque([0.0], maxlen=100)\n",
        "\n",
        "episode_count = 0\n",
        "\n",
        "# Implement TensorBoard logging\n",
        "summary_writer = SummaryWriter(LOG_DIR)\n",
        "\n",
        "online_net = Network(env, device=device)\n",
        "target_net = Network(env, device=device)\n",
        "\n",
        "# Delegate networks to GPU (if device = 'cpu' this does nothing)\n",
        "online_net = online_net.to(device)\n",
        "target_net = online_net.to(device)\n",
        "\n",
        "# When at risk of OOM errors enable RELOAD_PARAMS to get back to last checkpoint\n",
        "if RELOAD_PARAMS:\n",
        "   online_net.load(SAVE_PATH)\n",
        "\n",
        "# We set the target net parameters equal to the online_net params\n",
        "# As specified in \"Human-level control through deep reinforcement learning\"\n",
        "target_net.load_state_dict(online_net.state_dict())\n",
        "\n",
        "# Create optimizer for gradient descent\n",
        "optimizer = torch.optim.Adam(online_net.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Initialize Replay Buffer\n",
        "obses = env.reset()\n",
        "\n",
        "# If do_init_action[i] is True then environment i executes action 1 (start game)\n",
        "do_init_action = [True for _ in range(N_ENVS)]\n",
        "\n",
        "for _ in range(MIN_REPLAY_SIZE):\n",
        "  # Select random actions\n",
        "  actions = [env.action_space.sample() for _ in range(N_ENVS)]\n",
        "\n",
        "  # In the breakout game, we need to call action 1 each time a new game starts\n",
        "  # to release the projectile from the player's platform.\n",
        "  # We can help the agent by performing this action for them.\n",
        "  if FORCE_START:\n",
        "    actions = [1 if do_init_action[i] else a for i, a in enumerate(actions)]\n",
        "\n",
        "  # We step the environment with the selected actions\n",
        "  new_obses, rewards, dones, infos = env.step(actions)\n",
        "  do_init_action = list(dones)\n",
        "\n",
        "  # We zip together all the info related to the current transition\n",
        "  # and iterate over the resulting collection.\n",
        "  # Experiences from all batches are grouped together in a common pool.\n",
        "  for obs, action, reward, done, new_obs, info in zip(obses, \n",
        "                                                actions, \n",
        "                                                rewards, \n",
        "                                                dones, \n",
        "                                                new_obses,\n",
        "                                                infos):\n",
        "    # We group all this info in a 'transition' tuple\n",
        "    # We put the tuple in the replay buffer to accumulate experience\n",
        "    # If an episode is done the VecEnv will env.reset() for us\n",
        "    transition = (obs, action, reward, done, new_obs)\n",
        "    replay_buffer.append(transition)\n",
        "\n",
        "  # We set the current observations as past obses for the new cycle\n",
        "  obses = new_obses\n",
        "\n",
        " \n",
        "\n",
        "# Main Training Loop\n",
        "# After the random-actions loop we reset the environment and start training\n",
        "obses = env.reset()\n",
        "do_init_action = [True for _ in range(N_ENVS)]\n",
        "\n",
        "# Step the loop forward with the itertools.count() int generator\n",
        "for step in itertools.count():\n",
        "\n",
        "  # Epsilon decays linearly in time until reaching its final value\n",
        "  epsilon = np.interp(step * N_ENVS, \n",
        "                      [0, EPSILON_DECAY],\n",
        "                      [EPSILON_START, EPSILON_END])\n",
        "  \n",
        "  # Get the actions from the online network.\n",
        "  # If we are using frame-stacking with the custom wrapper\n",
        "  # we unwrap observations and stack frames before passing them to net.act(...).\n",
        "  # Epsilon-greedy policy is implemented in the net.act method\n",
        "  if isinstance(obses[0], PytorchLazyFrames):\n",
        "    act_obses = np.stack([o.get_frames() for o in obses])\n",
        "    actions = online_net.act(act_obses, epsilon)\n",
        "  else:\n",
        "    actions = online_net.act(obses, epsilon)\n",
        "  \n",
        "  if FORCE_START:\n",
        "    actions = [1 if do_init_action[i] else a for i, a in enumerate(actions)]\n",
        "    \n",
        "  # The training loop goes on as in the random-actions regime\n",
        "  new_obses, rewards, dones, infos = env.step(actions)\n",
        "  do_init_action = list(dones)\n",
        "\n",
        "  for obs, action, reward, done, new_obs, info in zip(obses, \n",
        "                                                actions, \n",
        "                                                rewards, \n",
        "                                                dones, \n",
        "                                                new_obses,\n",
        "                                                infos):\n",
        "    transition = (obs, action, reward, done, new_obs)\n",
        "    replay_buffer.append(transition)\n",
        "\n",
        "    # When an episode is done we append the episode info to the buffer.\n",
        "    # We pass the info to TensorBoard for out-of-the-box interactive graphs.\n",
        "    if done:\n",
        "      ep_infos_buffer.append(info['episode'])\n",
        "      episode_count += 1\n",
        "\n",
        "  obses = new_obses\n",
        "\n",
        "  # Start Gradient Step\n",
        "  transitions = random.sample(replay_buffer, BATCH_SIZE)\n",
        "  # Compute loss\n",
        "  loss = online_net.compute_loss(transitions, target_net)\n",
        "  # Gradient Descent\n",
        "  optimizer.zero_grad()\n",
        "  # Back-Propagation\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # Update Target Network with the online net weights\n",
        "  if step % TARGET_UPDATE_FREQ == 0:\n",
        "    target_net.load_state_dict(online_net.state_dict())\n",
        "\n",
        "  # Logging\n",
        "  if step % LOG_INTERVAL == 0:\n",
        "    if isinstance(ep_infos_buffer[0], dict):  \n",
        "      reward_mean = np.mean([e['r'] for e in ep_infos_buffer]) or 0\n",
        "      length_mean = np.mean([e['l'] for e in ep_infos_buffer]) or 0\n",
        "      # Log data to TensorBoard graphs\n",
        "      summary_writer.add_scalar('AvgRew', reward_mean, global_step=step)\n",
        "      summary_writer.add_scalar('AvgEpLen', length_mean, global_step=step)\n",
        "      summary_writer.add_scalar('Episodes', episode_count)\n",
        "    else:\n",
        "      reward_mean = 'N/A'\n",
        "      length_mean = 'N/A'\n",
        "\n",
        "    print()\n",
        "    print('Step', step)\n",
        "    print('Avg Reward', reward_mean)\n",
        "    print('Avg Episode Length', length_mean)\n",
        "    print('Episodes', episode_count)\n",
        "\n",
        "  # Save Network Parameters\n",
        "  if step % SAVE_INTERVAL == 0 and step != 0:\n",
        "    print('Saving...')\n",
        "    online_net.save(SAVE_PATH)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jU2zTbXbOoRa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "94ad2c94-f123-43cf-e15b-e4b2366c9ae1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-7e98bc9dbcf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0;31m# We step the environment with the selected actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m   \u001b[0mnew_obses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m   \u001b[0mdo_init_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/deep-q-learning-utils/baselines_wrappers/vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \"\"\"\n\u001b[1;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/deep-q-learning-utils/pytorch_wrappers.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mobses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_frame\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_stacks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/deep-q-learning-utils/baselines_wrappers/dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m#    action = int(action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_rews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_infos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/deep-q-learning-utils/baselines_wrappers/monitor.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tried to step environment that needs reset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;34m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m         \u001b[0mstep_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_returns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_returns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;34m\"\"\"Modifies the reward using :meth:`self.reward` after the environment :meth:`env.step`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m         \u001b[0mstep_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_returns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_returns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;34m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m         \u001b[0mstep_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_returns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_returns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;34m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m         \u001b[0mstep_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_returns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_returns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/deep-q-learning-utils/baselines_wrappers/atari_wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwas_real_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# check current lives, make loss of life terminal,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/deep-q-learning-utils/baselines_wrappers/atari_wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# Note that the observation on the done=True frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# doesn't matter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mmax_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obs_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmax_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     38\u001b[0m def _amax(a, axis=None, out=None, keepdims=False,\n\u001b[1;32m     39\u001b[0m           initial=_NoValue, where=True):\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m def _amin(a, axis=None, out=None, keepdims=False,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graphs with TensorBoard"
      ],
      "metadata": {
        "id": "O19jDGbX4I9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir $LOG_DIR"
      ],
      "metadata": {
        "id": "Ze_oh1_u2yIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "erlBv8J_Ooak"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VbtL8_HrOokY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tbq0WozsOouV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eoNvcFyJOo2F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}